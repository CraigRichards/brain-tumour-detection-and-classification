{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as tvf\n",
    "import torchvision.transforms as tvtfms\n",
    "import operator as op\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from timm import create_model\n",
    "\n",
    "# For type hinting later on\n",
    "import collections\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = create_model(\"convnext_tiny\", pretrained=False, num_classes=2, in_chans=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head = nn.Sequential(\n",
    "#     nn.BatchNorm1d(768),\n",
    "#     nn.Dropout(0.25),\n",
    "#     nn.Linear(768, 512, bias=False),\n",
    "#     nn.ReLU(inplace=True),\n",
    "#     nn.BatchNorm1d(512),\n",
    "#     nn.Dropout(0.5),\n",
    "#     nn.Linear(512, 2, bias=False)\n",
    "# )\n",
    "\n",
    "# class CustomHead(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CustomHead, self).__init__()\n",
    "#         self.norm = nn.BatchNorm1d(768)\n",
    "#         self.fc = nn.Linear(768, 2, bias=True)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.norm(x)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# head = CustomHead()\n",
    "\n",
    "model = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(net, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27941/3427640226.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(\"models/convnext_tiny_best_model.pth\")\n"
     ]
    }
   ],
   "source": [
    "state = torch.load(\"models/convnext_tiny_best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stem.0.weight',\n",
       " 'stem.0.bias',\n",
       " 'stem.1.weight',\n",
       " 'stem.1.bias',\n",
       " 'stages.0.blocks.0.gamma',\n",
       " 'stages.0.blocks.0.conv_dw.weight',\n",
       " 'stages.0.blocks.0.conv_dw.bias',\n",
       " 'stages.0.blocks.0.norm.weight',\n",
       " 'stages.0.blocks.0.norm.bias',\n",
       " 'stages.0.blocks.0.mlp.fc1.weight',\n",
       " 'stages.0.blocks.0.mlp.fc1.bias',\n",
       " 'stages.0.blocks.0.mlp.fc2.weight',\n",
       " 'stages.0.blocks.0.mlp.fc2.bias',\n",
       " 'stages.0.blocks.1.gamma',\n",
       " 'stages.0.blocks.1.conv_dw.weight',\n",
       " 'stages.0.blocks.1.conv_dw.bias',\n",
       " 'stages.0.blocks.1.norm.weight',\n",
       " 'stages.0.blocks.1.norm.bias',\n",
       " 'stages.0.blocks.1.mlp.fc1.weight',\n",
       " 'stages.0.blocks.1.mlp.fc1.bias',\n",
       " 'stages.0.blocks.1.mlp.fc2.weight',\n",
       " 'stages.0.blocks.1.mlp.fc2.bias',\n",
       " 'stages.0.blocks.2.gamma',\n",
       " 'stages.0.blocks.2.conv_dw.weight',\n",
       " 'stages.0.blocks.2.conv_dw.bias',\n",
       " 'stages.0.blocks.2.norm.weight',\n",
       " 'stages.0.blocks.2.norm.bias',\n",
       " 'stages.0.blocks.2.mlp.fc1.weight',\n",
       " 'stages.0.blocks.2.mlp.fc1.bias',\n",
       " 'stages.0.blocks.2.mlp.fc2.weight',\n",
       " 'stages.0.blocks.2.mlp.fc2.bias',\n",
       " 'stages.1.downsample.0.weight',\n",
       " 'stages.1.downsample.0.bias',\n",
       " 'stages.1.downsample.1.weight',\n",
       " 'stages.1.downsample.1.bias',\n",
       " 'stages.1.blocks.0.gamma',\n",
       " 'stages.1.blocks.0.conv_dw.weight',\n",
       " 'stages.1.blocks.0.conv_dw.bias',\n",
       " 'stages.1.blocks.0.norm.weight',\n",
       " 'stages.1.blocks.0.norm.bias',\n",
       " 'stages.1.blocks.0.mlp.fc1.weight',\n",
       " 'stages.1.blocks.0.mlp.fc1.bias',\n",
       " 'stages.1.blocks.0.mlp.fc2.weight',\n",
       " 'stages.1.blocks.0.mlp.fc2.bias',\n",
       " 'stages.1.blocks.1.gamma',\n",
       " 'stages.1.blocks.1.conv_dw.weight',\n",
       " 'stages.1.blocks.1.conv_dw.bias',\n",
       " 'stages.1.blocks.1.norm.weight',\n",
       " 'stages.1.blocks.1.norm.bias',\n",
       " 'stages.1.blocks.1.mlp.fc1.weight',\n",
       " 'stages.1.blocks.1.mlp.fc1.bias',\n",
       " 'stages.1.blocks.1.mlp.fc2.weight',\n",
       " 'stages.1.blocks.1.mlp.fc2.bias',\n",
       " 'stages.1.blocks.2.gamma',\n",
       " 'stages.1.blocks.2.conv_dw.weight',\n",
       " 'stages.1.blocks.2.conv_dw.bias',\n",
       " 'stages.1.blocks.2.norm.weight',\n",
       " 'stages.1.blocks.2.norm.bias',\n",
       " 'stages.1.blocks.2.mlp.fc1.weight',\n",
       " 'stages.1.blocks.2.mlp.fc1.bias',\n",
       " 'stages.1.blocks.2.mlp.fc2.weight',\n",
       " 'stages.1.blocks.2.mlp.fc2.bias',\n",
       " 'stages.2.downsample.0.weight',\n",
       " 'stages.2.downsample.0.bias',\n",
       " 'stages.2.downsample.1.weight',\n",
       " 'stages.2.downsample.1.bias',\n",
       " 'stages.2.blocks.0.gamma',\n",
       " 'stages.2.blocks.0.conv_dw.weight',\n",
       " 'stages.2.blocks.0.conv_dw.bias',\n",
       " 'stages.2.blocks.0.norm.weight',\n",
       " 'stages.2.blocks.0.norm.bias',\n",
       " 'stages.2.blocks.0.mlp.fc1.weight',\n",
       " 'stages.2.blocks.0.mlp.fc1.bias',\n",
       " 'stages.2.blocks.0.mlp.fc2.weight',\n",
       " 'stages.2.blocks.0.mlp.fc2.bias',\n",
       " 'stages.2.blocks.1.gamma',\n",
       " 'stages.2.blocks.1.conv_dw.weight',\n",
       " 'stages.2.blocks.1.conv_dw.bias',\n",
       " 'stages.2.blocks.1.norm.weight',\n",
       " 'stages.2.blocks.1.norm.bias',\n",
       " 'stages.2.blocks.1.mlp.fc1.weight',\n",
       " 'stages.2.blocks.1.mlp.fc1.bias',\n",
       " 'stages.2.blocks.1.mlp.fc2.weight',\n",
       " 'stages.2.blocks.1.mlp.fc2.bias',\n",
       " 'stages.2.blocks.2.gamma',\n",
       " 'stages.2.blocks.2.conv_dw.weight',\n",
       " 'stages.2.blocks.2.conv_dw.bias',\n",
       " 'stages.2.blocks.2.norm.weight',\n",
       " 'stages.2.blocks.2.norm.bias',\n",
       " 'stages.2.blocks.2.mlp.fc1.weight',\n",
       " 'stages.2.blocks.2.mlp.fc1.bias',\n",
       " 'stages.2.blocks.2.mlp.fc2.weight',\n",
       " 'stages.2.blocks.2.mlp.fc2.bias',\n",
       " 'stages.2.blocks.3.gamma',\n",
       " 'stages.2.blocks.3.conv_dw.weight',\n",
       " 'stages.2.blocks.3.conv_dw.bias',\n",
       " 'stages.2.blocks.3.norm.weight',\n",
       " 'stages.2.blocks.3.norm.bias',\n",
       " 'stages.2.blocks.3.mlp.fc1.weight',\n",
       " 'stages.2.blocks.3.mlp.fc1.bias',\n",
       " 'stages.2.blocks.3.mlp.fc2.weight',\n",
       " 'stages.2.blocks.3.mlp.fc2.bias',\n",
       " 'stages.2.blocks.4.gamma',\n",
       " 'stages.2.blocks.4.conv_dw.weight',\n",
       " 'stages.2.blocks.4.conv_dw.bias',\n",
       " 'stages.2.blocks.4.norm.weight',\n",
       " 'stages.2.blocks.4.norm.bias',\n",
       " 'stages.2.blocks.4.mlp.fc1.weight',\n",
       " 'stages.2.blocks.4.mlp.fc1.bias',\n",
       " 'stages.2.blocks.4.mlp.fc2.weight',\n",
       " 'stages.2.blocks.4.mlp.fc2.bias',\n",
       " 'stages.2.blocks.5.gamma',\n",
       " 'stages.2.blocks.5.conv_dw.weight',\n",
       " 'stages.2.blocks.5.conv_dw.bias',\n",
       " 'stages.2.blocks.5.norm.weight',\n",
       " 'stages.2.blocks.5.norm.bias',\n",
       " 'stages.2.blocks.5.mlp.fc1.weight',\n",
       " 'stages.2.blocks.5.mlp.fc1.bias',\n",
       " 'stages.2.blocks.5.mlp.fc2.weight',\n",
       " 'stages.2.blocks.5.mlp.fc2.bias',\n",
       " 'stages.2.blocks.6.gamma',\n",
       " 'stages.2.blocks.6.conv_dw.weight',\n",
       " 'stages.2.blocks.6.conv_dw.bias',\n",
       " 'stages.2.blocks.6.norm.weight',\n",
       " 'stages.2.blocks.6.norm.bias',\n",
       " 'stages.2.blocks.6.mlp.fc1.weight',\n",
       " 'stages.2.blocks.6.mlp.fc1.bias',\n",
       " 'stages.2.blocks.6.mlp.fc2.weight',\n",
       " 'stages.2.blocks.6.mlp.fc2.bias',\n",
       " 'stages.2.blocks.7.gamma',\n",
       " 'stages.2.blocks.7.conv_dw.weight',\n",
       " 'stages.2.blocks.7.conv_dw.bias',\n",
       " 'stages.2.blocks.7.norm.weight',\n",
       " 'stages.2.blocks.7.norm.bias',\n",
       " 'stages.2.blocks.7.mlp.fc1.weight',\n",
       " 'stages.2.blocks.7.mlp.fc1.bias',\n",
       " 'stages.2.blocks.7.mlp.fc2.weight',\n",
       " 'stages.2.blocks.7.mlp.fc2.bias',\n",
       " 'stages.2.blocks.8.gamma',\n",
       " 'stages.2.blocks.8.conv_dw.weight',\n",
       " 'stages.2.blocks.8.conv_dw.bias',\n",
       " 'stages.2.blocks.8.norm.weight',\n",
       " 'stages.2.blocks.8.norm.bias',\n",
       " 'stages.2.blocks.8.mlp.fc1.weight',\n",
       " 'stages.2.blocks.8.mlp.fc1.bias',\n",
       " 'stages.2.blocks.8.mlp.fc2.weight',\n",
       " 'stages.2.blocks.8.mlp.fc2.bias',\n",
       " 'stages.3.downsample.0.weight',\n",
       " 'stages.3.downsample.0.bias',\n",
       " 'stages.3.downsample.1.weight',\n",
       " 'stages.3.downsample.1.bias',\n",
       " 'stages.3.blocks.0.gamma',\n",
       " 'stages.3.blocks.0.conv_dw.weight',\n",
       " 'stages.3.blocks.0.conv_dw.bias',\n",
       " 'stages.3.blocks.0.norm.weight',\n",
       " 'stages.3.blocks.0.norm.bias',\n",
       " 'stages.3.blocks.0.mlp.fc1.weight',\n",
       " 'stages.3.blocks.0.mlp.fc1.bias',\n",
       " 'stages.3.blocks.0.mlp.fc2.weight',\n",
       " 'stages.3.blocks.0.mlp.fc2.bias',\n",
       " 'stages.3.blocks.1.gamma',\n",
       " 'stages.3.blocks.1.conv_dw.weight',\n",
       " 'stages.3.blocks.1.conv_dw.bias',\n",
       " 'stages.3.blocks.1.norm.weight',\n",
       " 'stages.3.blocks.1.norm.bias',\n",
       " 'stages.3.blocks.1.mlp.fc1.weight',\n",
       " 'stages.3.blocks.1.mlp.fc1.bias',\n",
       " 'stages.3.blocks.1.mlp.fc2.weight',\n",
       " 'stages.3.blocks.1.mlp.fc2.bias',\n",
       " 'stages.3.blocks.2.gamma',\n",
       " 'stages.3.blocks.2.conv_dw.weight',\n",
       " 'stages.3.blocks.2.conv_dw.bias',\n",
       " 'stages.3.blocks.2.norm.weight',\n",
       " 'stages.3.blocks.2.norm.bias',\n",
       " 'stages.3.blocks.2.mlp.fc1.weight',\n",
       " 'stages.3.blocks.2.mlp.fc1.bias',\n",
       " 'stages.3.blocks.2.mlp.fc2.weight',\n",
       " 'stages.3.blocks.2.mlp.fc2.bias',\n",
       " 'head.norm.weight',\n",
       " 'head.norm.bias',\n",
       " 'head.fc.weight',\n",
       " 'head.fc.bias']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stages.3.blocks.2.conv_dw.weight',\n",
       " 'stages.3.blocks.2.conv_dw.bias',\n",
       " 'stages.3.blocks.2.norm.weight',\n",
       " 'stages.3.blocks.2.norm.bias',\n",
       " 'stages.3.blocks.2.mlp.fc1.weight',\n",
       " 'stages.3.blocks.2.mlp.fc1.bias',\n",
       " 'stages.3.blocks.2.mlp.fc2.weight',\n",
       " 'stages.3.blocks.2.mlp.fc2.bias',\n",
       " 'head.norm.weight',\n",
       " 'head.norm.bias',\n",
       " 'head.fc.weight',\n",
       " 'head.fc.bias']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.state_dict().keys())[-12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stem.0.weight',\n",
       " 'stem.0.bias',\n",
       " 'stem.1.weight',\n",
       " 'stem.1.bias',\n",
       " 'stages.0.blocks.0.gamma',\n",
       " 'stages.0.blocks.0.conv_dw.weight',\n",
       " 'stages.0.blocks.0.conv_dw.bias',\n",
       " 'stages.0.blocks.0.norm.weight',\n",
       " 'stages.0.blocks.0.norm.bias',\n",
       " 'stages.0.blocks.0.mlp.fc1.weight',\n",
       " 'stages.0.blocks.0.mlp.fc1.bias',\n",
       " 'stages.0.blocks.0.mlp.fc2.weight',\n",
       " 'stages.0.blocks.0.mlp.fc2.bias',\n",
       " 'stages.0.blocks.1.gamma',\n",
       " 'stages.0.blocks.1.conv_dw.weight',\n",
       " 'stages.0.blocks.1.conv_dw.bias',\n",
       " 'stages.0.blocks.1.norm.weight',\n",
       " 'stages.0.blocks.1.norm.bias',\n",
       " 'stages.0.blocks.1.mlp.fc1.weight',\n",
       " 'stages.0.blocks.1.mlp.fc1.bias',\n",
       " 'stages.0.blocks.1.mlp.fc2.weight',\n",
       " 'stages.0.blocks.1.mlp.fc2.bias',\n",
       " 'stages.0.blocks.2.gamma',\n",
       " 'stages.0.blocks.2.conv_dw.weight',\n",
       " 'stages.0.blocks.2.conv_dw.bias',\n",
       " 'stages.0.blocks.2.norm.weight',\n",
       " 'stages.0.blocks.2.norm.bias',\n",
       " 'stages.0.blocks.2.mlp.fc1.weight',\n",
       " 'stages.0.blocks.2.mlp.fc1.bias',\n",
       " 'stages.0.blocks.2.mlp.fc2.weight',\n",
       " 'stages.0.blocks.2.mlp.fc2.bias',\n",
       " 'stages.1.downsample.0.weight',\n",
       " 'stages.1.downsample.0.bias',\n",
       " 'stages.1.downsample.1.weight',\n",
       " 'stages.1.downsample.1.bias',\n",
       " 'stages.1.blocks.0.gamma',\n",
       " 'stages.1.blocks.0.conv_dw.weight',\n",
       " 'stages.1.blocks.0.conv_dw.bias',\n",
       " 'stages.1.blocks.0.norm.weight',\n",
       " 'stages.1.blocks.0.norm.bias',\n",
       " 'stages.1.blocks.0.mlp.fc1.weight',\n",
       " 'stages.1.blocks.0.mlp.fc1.bias',\n",
       " 'stages.1.blocks.0.mlp.fc2.weight',\n",
       " 'stages.1.blocks.0.mlp.fc2.bias',\n",
       " 'stages.1.blocks.1.gamma',\n",
       " 'stages.1.blocks.1.conv_dw.weight',\n",
       " 'stages.1.blocks.1.conv_dw.bias',\n",
       " 'stages.1.blocks.1.norm.weight',\n",
       " 'stages.1.blocks.1.norm.bias',\n",
       " 'stages.1.blocks.1.mlp.fc1.weight',\n",
       " 'stages.1.blocks.1.mlp.fc1.bias',\n",
       " 'stages.1.blocks.1.mlp.fc2.weight',\n",
       " 'stages.1.blocks.1.mlp.fc2.bias',\n",
       " 'stages.1.blocks.2.gamma',\n",
       " 'stages.1.blocks.2.conv_dw.weight',\n",
       " 'stages.1.blocks.2.conv_dw.bias',\n",
       " 'stages.1.blocks.2.norm.weight',\n",
       " 'stages.1.blocks.2.norm.bias',\n",
       " 'stages.1.blocks.2.mlp.fc1.weight',\n",
       " 'stages.1.blocks.2.mlp.fc1.bias',\n",
       " 'stages.1.blocks.2.mlp.fc2.weight',\n",
       " 'stages.1.blocks.2.mlp.fc2.bias',\n",
       " 'stages.2.downsample.0.weight',\n",
       " 'stages.2.downsample.0.bias',\n",
       " 'stages.2.downsample.1.weight',\n",
       " 'stages.2.downsample.1.bias',\n",
       " 'stages.2.blocks.0.gamma',\n",
       " 'stages.2.blocks.0.conv_dw.weight',\n",
       " 'stages.2.blocks.0.conv_dw.bias',\n",
       " 'stages.2.blocks.0.norm.weight',\n",
       " 'stages.2.blocks.0.norm.bias',\n",
       " 'stages.2.blocks.0.mlp.fc1.weight',\n",
       " 'stages.2.blocks.0.mlp.fc1.bias',\n",
       " 'stages.2.blocks.0.mlp.fc2.weight',\n",
       " 'stages.2.blocks.0.mlp.fc2.bias',\n",
       " 'stages.2.blocks.1.gamma',\n",
       " 'stages.2.blocks.1.conv_dw.weight',\n",
       " 'stages.2.blocks.1.conv_dw.bias',\n",
       " 'stages.2.blocks.1.norm.weight',\n",
       " 'stages.2.blocks.1.norm.bias',\n",
       " 'stages.2.blocks.1.mlp.fc1.weight',\n",
       " 'stages.2.blocks.1.mlp.fc1.bias',\n",
       " 'stages.2.blocks.1.mlp.fc2.weight',\n",
       " 'stages.2.blocks.1.mlp.fc2.bias',\n",
       " 'stages.2.blocks.2.gamma',\n",
       " 'stages.2.blocks.2.conv_dw.weight',\n",
       " 'stages.2.blocks.2.conv_dw.bias',\n",
       " 'stages.2.blocks.2.norm.weight',\n",
       " 'stages.2.blocks.2.norm.bias',\n",
       " 'stages.2.blocks.2.mlp.fc1.weight',\n",
       " 'stages.2.blocks.2.mlp.fc1.bias',\n",
       " 'stages.2.blocks.2.mlp.fc2.weight',\n",
       " 'stages.2.blocks.2.mlp.fc2.bias',\n",
       " 'stages.2.blocks.3.gamma',\n",
       " 'stages.2.blocks.3.conv_dw.weight',\n",
       " 'stages.2.blocks.3.conv_dw.bias',\n",
       " 'stages.2.blocks.3.norm.weight',\n",
       " 'stages.2.blocks.3.norm.bias',\n",
       " 'stages.2.blocks.3.mlp.fc1.weight',\n",
       " 'stages.2.blocks.3.mlp.fc1.bias',\n",
       " 'stages.2.blocks.3.mlp.fc2.weight',\n",
       " 'stages.2.blocks.3.mlp.fc2.bias',\n",
       " 'stages.2.blocks.4.gamma',\n",
       " 'stages.2.blocks.4.conv_dw.weight',\n",
       " 'stages.2.blocks.4.conv_dw.bias',\n",
       " 'stages.2.blocks.4.norm.weight',\n",
       " 'stages.2.blocks.4.norm.bias',\n",
       " 'stages.2.blocks.4.mlp.fc1.weight',\n",
       " 'stages.2.blocks.4.mlp.fc1.bias',\n",
       " 'stages.2.blocks.4.mlp.fc2.weight',\n",
       " 'stages.2.blocks.4.mlp.fc2.bias',\n",
       " 'stages.2.blocks.5.gamma',\n",
       " 'stages.2.blocks.5.conv_dw.weight',\n",
       " 'stages.2.blocks.5.conv_dw.bias',\n",
       " 'stages.2.blocks.5.norm.weight',\n",
       " 'stages.2.blocks.5.norm.bias',\n",
       " 'stages.2.blocks.5.mlp.fc1.weight',\n",
       " 'stages.2.blocks.5.mlp.fc1.bias',\n",
       " 'stages.2.blocks.5.mlp.fc2.weight',\n",
       " 'stages.2.blocks.5.mlp.fc2.bias',\n",
       " 'stages.2.blocks.6.gamma',\n",
       " 'stages.2.blocks.6.conv_dw.weight',\n",
       " 'stages.2.blocks.6.conv_dw.bias',\n",
       " 'stages.2.blocks.6.norm.weight',\n",
       " 'stages.2.blocks.6.norm.bias',\n",
       " 'stages.2.blocks.6.mlp.fc1.weight',\n",
       " 'stages.2.blocks.6.mlp.fc1.bias',\n",
       " 'stages.2.blocks.6.mlp.fc2.weight',\n",
       " 'stages.2.blocks.6.mlp.fc2.bias',\n",
       " 'stages.2.blocks.7.gamma',\n",
       " 'stages.2.blocks.7.conv_dw.weight',\n",
       " 'stages.2.blocks.7.conv_dw.bias',\n",
       " 'stages.2.blocks.7.norm.weight',\n",
       " 'stages.2.blocks.7.norm.bias',\n",
       " 'stages.2.blocks.7.mlp.fc1.weight',\n",
       " 'stages.2.blocks.7.mlp.fc1.bias',\n",
       " 'stages.2.blocks.7.mlp.fc2.weight',\n",
       " 'stages.2.blocks.7.mlp.fc2.bias',\n",
       " 'stages.2.blocks.8.gamma',\n",
       " 'stages.2.blocks.8.conv_dw.weight',\n",
       " 'stages.2.blocks.8.conv_dw.bias',\n",
       " 'stages.2.blocks.8.norm.weight',\n",
       " 'stages.2.blocks.8.norm.bias',\n",
       " 'stages.2.blocks.8.mlp.fc1.weight',\n",
       " 'stages.2.blocks.8.mlp.fc1.bias',\n",
       " 'stages.2.blocks.8.mlp.fc2.weight',\n",
       " 'stages.2.blocks.8.mlp.fc2.bias',\n",
       " 'stages.3.downsample.0.weight',\n",
       " 'stages.3.downsample.0.bias',\n",
       " 'stages.3.downsample.1.weight',\n",
       " 'stages.3.downsample.1.bias',\n",
       " 'stages.3.blocks.0.gamma',\n",
       " 'stages.3.blocks.0.conv_dw.weight',\n",
       " 'stages.3.blocks.0.conv_dw.bias',\n",
       " 'stages.3.blocks.0.norm.weight',\n",
       " 'stages.3.blocks.0.norm.bias',\n",
       " 'stages.3.blocks.0.mlp.fc1.weight',\n",
       " 'stages.3.blocks.0.mlp.fc1.bias',\n",
       " 'stages.3.blocks.0.mlp.fc2.weight',\n",
       " 'stages.3.blocks.0.mlp.fc2.bias',\n",
       " 'stages.3.blocks.1.gamma',\n",
       " 'stages.3.blocks.1.conv_dw.weight',\n",
       " 'stages.3.blocks.1.conv_dw.bias',\n",
       " 'stages.3.blocks.1.norm.weight',\n",
       " 'stages.3.blocks.1.norm.bias',\n",
       " 'stages.3.blocks.1.mlp.fc1.weight',\n",
       " 'stages.3.blocks.1.mlp.fc1.bias',\n",
       " 'stages.3.blocks.1.mlp.fc2.weight',\n",
       " 'stages.3.blocks.1.mlp.fc2.bias',\n",
       " 'stages.3.blocks.2.gamma',\n",
       " 'stages.3.blocks.2.conv_dw.weight',\n",
       " 'stages.3.blocks.2.conv_dw.bias',\n",
       " 'stages.3.blocks.2.norm.weight',\n",
       " 'stages.3.blocks.2.norm.bias',\n",
       " 'stages.3.blocks.2.mlp.fc1.weight',\n",
       " 'stages.3.blocks.2.mlp.fc1.bias',\n",
       " 'stages.3.blocks.2.mlp.fc2.weight',\n",
       " 'stages.3.blocks.2.mlp.fc2.bias',\n",
       " 'head.norm.weight',\n",
       " 'head.norm.bias',\n",
       " 'head.fc.weight',\n",
       " 'head.fc.bias']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(state.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stages.3.blocks.2.mlp.fc1.weight',\n",
       " 'stages.3.blocks.2.mlp.fc1.bias',\n",
       " 'stages.3.blocks.2.mlp.fc2.weight',\n",
       " 'stages.3.blocks.2.mlp.fc2.bias',\n",
       " 'head.norm.weight',\n",
       " 'head.norm.bias',\n",
       " 'head.fc.weight',\n",
       " 'head.fc.bias']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(state.keys())[-8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected to fail\n",
    "model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def copy_weight(name, parameter, state_dict):\n",
    "#     \"\"\"\n",
    "#     Takes in a layer `name`, model `parameter`, and `state_dict`\n",
    "#     and loads the weights from `state_dict` into `parameter`\n",
    "#     if it exists.\n",
    "#     \"\"\"\n",
    "\n",
    "#     if name[0] in [\"0\", \"1\"]:\n",
    "#         # name = name[:2] + \"model.\" + name[2:]\n",
    "#         name = name[2:]\n",
    "#     if name in state_dict.keys():\n",
    "#         input_parameter = state_dict[name]\n",
    "#         if input_parameter.shape == parameter.shape:\n",
    "#             parameter.copy_(input_parameter)\n",
    "#         else:\n",
    "#             print(f'Shape mismatch at layer: {name}, skipping')\n",
    "#     else:\n",
    "#         print(f'{name} is not in the state_dict, skipping.')\n",
    "           \n",
    "#     # if name[0] == \"0\":\n",
    "#     #     name = name[:2] + \"model.\" + name[2:]\n",
    "#     # if name in state_dict.keys():\n",
    "#     #     input_parameter = state_dict[name]\n",
    "#     #     if input_parameter.shape == parameter.shape:\n",
    "#     #         parameter.copy_(input_parameter)\n",
    "#     #     else:\n",
    "#     #         print(f'Shape mismatch at layer: {name}, skipping')\n",
    "#     # else:\n",
    "#     #     print(f'{name} is not in the state_dict, skipping.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_weights(input_model:nn.Module, input_weights:collections.OrderedDict, application_function:callable):\n",
    "#     \"\"\"\n",
    "#     Takes an input state_dict and applies those weights to the `input_model`, potentially \n",
    "#     with a modifier function.\n",
    "    \n",
    "#     Args:\n",
    "#         input_model (`nn.Module`):\n",
    "#             The model that weights should be applied to\n",
    "#         input_weights (`collections.OrderedDict`):\n",
    "#             A dictionary of weights, the trained model's `state_dict()`\n",
    "#         application_function (`callable`):\n",
    "#             A function that takes in one parameter and layer name from `input_model`\n",
    "#             and the `input_weights`. Should apply the weights from the state dict into `input_model`.\n",
    "#     \"\"\"\n",
    "#     model_dict = input_model.state_dict()\n",
    "#     for name, parameter in model_dict.items():\n",
    "#         application_function(name, parameter, input_weights)\n",
    "#     input_model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply_weights(model, state, copy_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class index: 1\n",
      "Predicted class label: yes\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# define the labels as the came from dls.vocab\n",
    "class_labels = ['no', 'yes']\n",
    "\n",
    "# Define the image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image to the size expected by the model\n",
    "    transforms.ToTensor(),          # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "# Load the image\n",
    "image_path = 'processed/valid/yes/Y1.jpg'\n",
    "image = Image.open(image_path)\n",
    "image = transform(image).unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Run the image through the model\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "\n",
    "# Get the predicted class\n",
    "_, predicted_class = torch.max(output, 1)\n",
    "predicted_index = predicted_class.item()\n",
    "\n",
    "# Map the index to the class label\n",
    "predicted_label = class_labels[predicted_index]\n",
    "\n",
    "print(f'Predicted class index: {predicted_index}')\n",
    "print(f'Predicted class label: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Image: 25 no.jpg\n",
      "Image shape: (232, 217)\n",
      "Image channels: 3\n",
      "Image: 25 no.jpg\n",
      "Predicted class index: 0\n",
      "Predicted class label: no\n",
      "---\n",
      "Processing Image: 28 no.jpg\n",
      "Image shape: (201, 251)\n",
      "Image channels: 3\n",
      "Image: 28 no.jpg\n",
      "Predicted class index: 1\n",
      "Predicted class label: yes\n",
      "---\n",
      "Processing Image: 33 no.jpg\n",
      "Image shape: (236, 213)\n",
      "Image channels: 3\n",
      "Image: 33 no.jpg\n",
      "Predicted class index: 0\n",
      "Predicted class label: no\n",
      "---\n",
      "Processing Image: N1.JPG\n",
      "Image shape: (276, 338)\n",
      "Image channels: 3\n",
      "Image: N1.JPG\n",
      "Predicted class index: 1\n",
      "Predicted class label: yes\n",
      "---\n",
      "Processing Image: N15.jpg\n",
      "Image shape: (225, 225)\n",
      "Image channels: 1\n",
      "Image: N15.jpg\n",
      "Predicted class index: 0\n",
      "Predicted class label: no\n",
      "---\n",
      "Processing Image: no 4.jpg\n",
      "Image shape: (220, 275)\n",
      "Image channels: 1\n",
      "Image: no 4.jpg\n",
      "Predicted class index: 0\n",
      "Predicted class label: no\n",
      "---\n",
      "Processing Image: no 92.jpg\n",
      "Image shape: (206, 244)\n",
      "Image channels: 3\n",
      "Image: no 92.jpg\n",
      "Predicted class index: 0\n",
      "Predicted class label: no\n",
      "---\n",
      "Processing Image: no 94.jpg\n",
      "Image shape: (630, 630)\n",
      "Image channels: 1\n",
      "Image: no 94.jpg\n",
      "Predicted class index: 0\n",
      "Predicted class label: no\n",
      "---\n",
      "Processing Image: No14.jpg\n",
      "Image shape: (339, 340)\n",
      "Image channels: 3\n",
      "Image: No14.jpg\n",
      "Predicted class index: 0\n",
      "Predicted class label: no\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define the labels as they came from dls.vocab\n",
    "class_labels = ['no', 'yes']\n",
    "\n",
    "# Define the image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image to the size expected by the model\n",
    "    transforms.ToTensor(),          # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "# Path to the \"no\" folder\n",
    "folder_path = 'processed/valid/no'\n",
    "\n",
    "# List all image files in the \"no\" folder\n",
    "image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Loop through each image file\n",
    "for image_file in image_files:\n",
    "    print(f'Processing Image: {image_file}')\n",
    "    image_path = os.path.join(folder_path, image_file)\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    print(f'Image shape: {image.size}')\n",
    "    # print no of channels\n",
    "    print(f'Image channels: {len(image.getbands())}')\n",
    "    \n",
    "    # Convert single-channel images to three-channel\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    image = transform(image).unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "    # Run the image through the model\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "\n",
    "    # Get the predicted class\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "    predicted_index = predicted_class.item()\n",
    "\n",
    "    # Map the index to the class label\n",
    "    predicted_label = class_labels[predicted_index]\n",
    "\n",
    "    print(f'Image: {image_file}')\n",
    "    print(f'Predicted class index: {predicted_index}')\n",
    "    print(f'Predicted class label: {predicted_label}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
