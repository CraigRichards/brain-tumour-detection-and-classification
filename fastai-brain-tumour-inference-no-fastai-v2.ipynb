{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as tvf\n",
    "import torchvision.transforms as tvtfms\n",
    "import operator as op\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from timm import create_model\n",
    "\n",
    "# For type hinting later on\n",
    "import collections\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes must match the vocab\n",
    "model = create_model(\"convnext_tiny\", pretrained=False, num_classes=2, in_chans=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78051/3427640226.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(\"models/convnext_tiny_best_model.pth\")\n"
     ]
    }
   ],
   "source": [
    "state = torch.load(\"models/convnext_tiny_best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stem.0.weight',\n",
       " 'stem.0.bias',\n",
       " 'stem.1.weight',\n",
       " 'stem.1.bias',\n",
       " 'stages.0.blocks.0.gamma',\n",
       " 'stages.0.blocks.0.conv_dw.weight',\n",
       " 'stages.0.blocks.0.conv_dw.bias',\n",
       " 'stages.0.blocks.0.norm.weight',\n",
       " 'stages.0.blocks.0.norm.bias',\n",
       " 'stages.0.blocks.0.mlp.fc1.weight',\n",
       " 'stages.0.blocks.0.mlp.fc1.bias',\n",
       " 'stages.0.blocks.0.mlp.fc2.weight',\n",
       " 'stages.0.blocks.0.mlp.fc2.bias',\n",
       " 'stages.0.blocks.1.gamma',\n",
       " 'stages.0.blocks.1.conv_dw.weight',\n",
       " 'stages.0.blocks.1.conv_dw.bias',\n",
       " 'stages.0.blocks.1.norm.weight',\n",
       " 'stages.0.blocks.1.norm.bias',\n",
       " 'stages.0.blocks.1.mlp.fc1.weight',\n",
       " 'stages.0.blocks.1.mlp.fc1.bias',\n",
       " 'stages.0.blocks.1.mlp.fc2.weight',\n",
       " 'stages.0.blocks.1.mlp.fc2.bias',\n",
       " 'stages.0.blocks.2.gamma',\n",
       " 'stages.0.blocks.2.conv_dw.weight',\n",
       " 'stages.0.blocks.2.conv_dw.bias',\n",
       " 'stages.0.blocks.2.norm.weight',\n",
       " 'stages.0.blocks.2.norm.bias',\n",
       " 'stages.0.blocks.2.mlp.fc1.weight',\n",
       " 'stages.0.blocks.2.mlp.fc1.bias',\n",
       " 'stages.0.blocks.2.mlp.fc2.weight',\n",
       " 'stages.0.blocks.2.mlp.fc2.bias',\n",
       " 'stages.1.downsample.0.weight',\n",
       " 'stages.1.downsample.0.bias',\n",
       " 'stages.1.downsample.1.weight',\n",
       " 'stages.1.downsample.1.bias',\n",
       " 'stages.1.blocks.0.gamma',\n",
       " 'stages.1.blocks.0.conv_dw.weight',\n",
       " 'stages.1.blocks.0.conv_dw.bias',\n",
       " 'stages.1.blocks.0.norm.weight',\n",
       " 'stages.1.blocks.0.norm.bias',\n",
       " 'stages.1.blocks.0.mlp.fc1.weight',\n",
       " 'stages.1.blocks.0.mlp.fc1.bias',\n",
       " 'stages.1.blocks.0.mlp.fc2.weight',\n",
       " 'stages.1.blocks.0.mlp.fc2.bias',\n",
       " 'stages.1.blocks.1.gamma',\n",
       " 'stages.1.blocks.1.conv_dw.weight',\n",
       " 'stages.1.blocks.1.conv_dw.bias',\n",
       " 'stages.1.blocks.1.norm.weight',\n",
       " 'stages.1.blocks.1.norm.bias',\n",
       " 'stages.1.blocks.1.mlp.fc1.weight',\n",
       " 'stages.1.blocks.1.mlp.fc1.bias',\n",
       " 'stages.1.blocks.1.mlp.fc2.weight',\n",
       " 'stages.1.blocks.1.mlp.fc2.bias',\n",
       " 'stages.1.blocks.2.gamma',\n",
       " 'stages.1.blocks.2.conv_dw.weight',\n",
       " 'stages.1.blocks.2.conv_dw.bias',\n",
       " 'stages.1.blocks.2.norm.weight',\n",
       " 'stages.1.blocks.2.norm.bias',\n",
       " 'stages.1.blocks.2.mlp.fc1.weight',\n",
       " 'stages.1.blocks.2.mlp.fc1.bias',\n",
       " 'stages.1.blocks.2.mlp.fc2.weight',\n",
       " 'stages.1.blocks.2.mlp.fc2.bias',\n",
       " 'stages.2.downsample.0.weight',\n",
       " 'stages.2.downsample.0.bias',\n",
       " 'stages.2.downsample.1.weight',\n",
       " 'stages.2.downsample.1.bias',\n",
       " 'stages.2.blocks.0.gamma',\n",
       " 'stages.2.blocks.0.conv_dw.weight',\n",
       " 'stages.2.blocks.0.conv_dw.bias',\n",
       " 'stages.2.blocks.0.norm.weight',\n",
       " 'stages.2.blocks.0.norm.bias',\n",
       " 'stages.2.blocks.0.mlp.fc1.weight',\n",
       " 'stages.2.blocks.0.mlp.fc1.bias',\n",
       " 'stages.2.blocks.0.mlp.fc2.weight',\n",
       " 'stages.2.blocks.0.mlp.fc2.bias',\n",
       " 'stages.2.blocks.1.gamma',\n",
       " 'stages.2.blocks.1.conv_dw.weight',\n",
       " 'stages.2.blocks.1.conv_dw.bias',\n",
       " 'stages.2.blocks.1.norm.weight',\n",
       " 'stages.2.blocks.1.norm.bias',\n",
       " 'stages.2.blocks.1.mlp.fc1.weight',\n",
       " 'stages.2.blocks.1.mlp.fc1.bias',\n",
       " 'stages.2.blocks.1.mlp.fc2.weight',\n",
       " 'stages.2.blocks.1.mlp.fc2.bias',\n",
       " 'stages.2.blocks.2.gamma',\n",
       " 'stages.2.blocks.2.conv_dw.weight',\n",
       " 'stages.2.blocks.2.conv_dw.bias',\n",
       " 'stages.2.blocks.2.norm.weight',\n",
       " 'stages.2.blocks.2.norm.bias',\n",
       " 'stages.2.blocks.2.mlp.fc1.weight',\n",
       " 'stages.2.blocks.2.mlp.fc1.bias',\n",
       " 'stages.2.blocks.2.mlp.fc2.weight',\n",
       " 'stages.2.blocks.2.mlp.fc2.bias',\n",
       " 'stages.2.blocks.3.gamma',\n",
       " 'stages.2.blocks.3.conv_dw.weight',\n",
       " 'stages.2.blocks.3.conv_dw.bias',\n",
       " 'stages.2.blocks.3.norm.weight',\n",
       " 'stages.2.blocks.3.norm.bias',\n",
       " 'stages.2.blocks.3.mlp.fc1.weight',\n",
       " 'stages.2.blocks.3.mlp.fc1.bias',\n",
       " 'stages.2.blocks.3.mlp.fc2.weight',\n",
       " 'stages.2.blocks.3.mlp.fc2.bias',\n",
       " 'stages.2.blocks.4.gamma',\n",
       " 'stages.2.blocks.4.conv_dw.weight',\n",
       " 'stages.2.blocks.4.conv_dw.bias',\n",
       " 'stages.2.blocks.4.norm.weight',\n",
       " 'stages.2.blocks.4.norm.bias',\n",
       " 'stages.2.blocks.4.mlp.fc1.weight',\n",
       " 'stages.2.blocks.4.mlp.fc1.bias',\n",
       " 'stages.2.blocks.4.mlp.fc2.weight',\n",
       " 'stages.2.blocks.4.mlp.fc2.bias',\n",
       " 'stages.2.blocks.5.gamma',\n",
       " 'stages.2.blocks.5.conv_dw.weight',\n",
       " 'stages.2.blocks.5.conv_dw.bias',\n",
       " 'stages.2.blocks.5.norm.weight',\n",
       " 'stages.2.blocks.5.norm.bias',\n",
       " 'stages.2.blocks.5.mlp.fc1.weight',\n",
       " 'stages.2.blocks.5.mlp.fc1.bias',\n",
       " 'stages.2.blocks.5.mlp.fc2.weight',\n",
       " 'stages.2.blocks.5.mlp.fc2.bias',\n",
       " 'stages.2.blocks.6.gamma',\n",
       " 'stages.2.blocks.6.conv_dw.weight',\n",
       " 'stages.2.blocks.6.conv_dw.bias',\n",
       " 'stages.2.blocks.6.norm.weight',\n",
       " 'stages.2.blocks.6.norm.bias',\n",
       " 'stages.2.blocks.6.mlp.fc1.weight',\n",
       " 'stages.2.blocks.6.mlp.fc1.bias',\n",
       " 'stages.2.blocks.6.mlp.fc2.weight',\n",
       " 'stages.2.blocks.6.mlp.fc2.bias',\n",
       " 'stages.2.blocks.7.gamma',\n",
       " 'stages.2.blocks.7.conv_dw.weight',\n",
       " 'stages.2.blocks.7.conv_dw.bias',\n",
       " 'stages.2.blocks.7.norm.weight',\n",
       " 'stages.2.blocks.7.norm.bias',\n",
       " 'stages.2.blocks.7.mlp.fc1.weight',\n",
       " 'stages.2.blocks.7.mlp.fc1.bias',\n",
       " 'stages.2.blocks.7.mlp.fc2.weight',\n",
       " 'stages.2.blocks.7.mlp.fc2.bias',\n",
       " 'stages.2.blocks.8.gamma',\n",
       " 'stages.2.blocks.8.conv_dw.weight',\n",
       " 'stages.2.blocks.8.conv_dw.bias',\n",
       " 'stages.2.blocks.8.norm.weight',\n",
       " 'stages.2.blocks.8.norm.bias',\n",
       " 'stages.2.blocks.8.mlp.fc1.weight',\n",
       " 'stages.2.blocks.8.mlp.fc1.bias',\n",
       " 'stages.2.blocks.8.mlp.fc2.weight',\n",
       " 'stages.2.blocks.8.mlp.fc2.bias',\n",
       " 'stages.3.downsample.0.weight',\n",
       " 'stages.3.downsample.0.bias',\n",
       " 'stages.3.downsample.1.weight',\n",
       " 'stages.3.downsample.1.bias',\n",
       " 'stages.3.blocks.0.gamma',\n",
       " 'stages.3.blocks.0.conv_dw.weight',\n",
       " 'stages.3.blocks.0.conv_dw.bias',\n",
       " 'stages.3.blocks.0.norm.weight',\n",
       " 'stages.3.blocks.0.norm.bias',\n",
       " 'stages.3.blocks.0.mlp.fc1.weight',\n",
       " 'stages.3.blocks.0.mlp.fc1.bias',\n",
       " 'stages.3.blocks.0.mlp.fc2.weight',\n",
       " 'stages.3.blocks.0.mlp.fc2.bias',\n",
       " 'stages.3.blocks.1.gamma',\n",
       " 'stages.3.blocks.1.conv_dw.weight',\n",
       " 'stages.3.blocks.1.conv_dw.bias',\n",
       " 'stages.3.blocks.1.norm.weight',\n",
       " 'stages.3.blocks.1.norm.bias',\n",
       " 'stages.3.blocks.1.mlp.fc1.weight',\n",
       " 'stages.3.blocks.1.mlp.fc1.bias',\n",
       " 'stages.3.blocks.1.mlp.fc2.weight',\n",
       " 'stages.3.blocks.1.mlp.fc2.bias',\n",
       " 'stages.3.blocks.2.gamma',\n",
       " 'stages.3.blocks.2.conv_dw.weight',\n",
       " 'stages.3.blocks.2.conv_dw.bias',\n",
       " 'stages.3.blocks.2.norm.weight',\n",
       " 'stages.3.blocks.2.norm.bias',\n",
       " 'stages.3.blocks.2.mlp.fc1.weight',\n",
       " 'stages.3.blocks.2.mlp.fc1.bias',\n",
       " 'stages.3.blocks.2.mlp.fc2.weight',\n",
       " 'stages.3.blocks.2.mlp.fc2.bias',\n",
       " 'head.norm.weight',\n",
       " 'head.norm.bias',\n",
       " 'head.fc.weight',\n",
       " 'head.fc.bias']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stages.3.blocks.2.conv_dw.weight',\n",
       " 'stages.3.blocks.2.conv_dw.bias',\n",
       " 'stages.3.blocks.2.norm.weight',\n",
       " 'stages.3.blocks.2.norm.bias',\n",
       " 'stages.3.blocks.2.mlp.fc1.weight',\n",
       " 'stages.3.blocks.2.mlp.fc1.bias',\n",
       " 'stages.3.blocks.2.mlp.fc2.weight',\n",
       " 'stages.3.blocks.2.mlp.fc2.bias',\n",
       " 'head.norm.weight',\n",
       " 'head.norm.bias',\n",
       " 'head.fc.weight',\n",
       " 'head.fc.bias']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.state_dict().keys())[-12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stages.3.blocks.2.conv_dw.weight',\n",
       " 'stages.3.blocks.2.conv_dw.bias',\n",
       " 'stages.3.blocks.2.norm.weight',\n",
       " 'stages.3.blocks.2.norm.bias',\n",
       " 'stages.3.blocks.2.mlp.fc1.weight',\n",
       " 'stages.3.blocks.2.mlp.fc1.bias',\n",
       " 'stages.3.blocks.2.mlp.fc2.weight',\n",
       " 'stages.3.blocks.2.mlp.fc2.bias',\n",
       " 'head.norm.weight',\n",
       " 'head.norm.bias',\n",
       " 'head.fc.weight',\n",
       " 'head.fc.bias']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(state.keys())[-12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected to fail\n",
    "model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also works\n",
    "# model = torch.load('models/convnext_tiny_best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as tvf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image: typing.Union[Image.Image, torch.Tensor], size: typing.Tuple[int, int]) -> typing.Union[Image.Image, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Takes a `PIL.Image` or `torch.Tensor` and crops it to `size` unless one \n",
    "    dimension is larger than the actual image. Padding must be performed afterwards if so.\n",
    "    \n",
    "    Args:\n",
    "        image (`PIL.Image` or `torch.Tensor`):\n",
    "            An image to perform cropping on\n",
    "        size (`tuple` of integers):\n",
    "            A size to crop to, should be in the form of (width, height)\n",
    "            \n",
    "    Returns:\n",
    "        An augmented `PIL.Image` or `torch.Tensor`\n",
    "    \"\"\"\n",
    "    if isinstance(image, Image.Image):\n",
    "        width, height = image.size\n",
    "    elif isinstance(image, torch.Tensor):\n",
    "        height, width = image.shape[-2], image.shape[-1]\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported image type\")\n",
    "\n",
    "    top = (height - size[1]) // 2\n",
    "    left = (width - size[0]) // 2\n",
    "    \n",
    "    top = max(top, 0)\n",
    "    left = max(left, 0)\n",
    "    \n",
    "    bottom = min(top + size[1], height)\n",
    "    right = min(left + size[0], width)\n",
    "\n",
    "    if isinstance(image, Image.Image):\n",
    "        return image.crop((left, top, right, bottom))\n",
    "    elif isinstance(image, torch.Tensor):\n",
    "        return image[:, :, top:bottom, left:right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "import typing\n",
    "\n",
    "def pad(image: typing.Union[Image.Image, torch.Tensor], size: typing.Tuple[int, int]) -> typing.Union[Image.Image, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Pads the given image to the specified size.\n",
    "    \n",
    "    Args:\n",
    "        image (`PIL.Image` or `torch.Tensor`):\n",
    "            An image to perform padding on\n",
    "        size (`tuple` of integers):\n",
    "            A size to pad to, should be in the form of (width, height)\n",
    "            \n",
    "    Returns:\n",
    "        A padded `PIL.Image` or `torch.Tensor`\n",
    "    \"\"\"\n",
    "    if isinstance(image, Image.Image):\n",
    "        width, height = image.size\n",
    "    elif isinstance(image, torch.Tensor):\n",
    "        height, width = image.shape[-2], image.shape[-1]\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported image type\")\n",
    "\n",
    "    top = (size[1] - height) // 2\n",
    "    left = (size[0] - width) // 2\n",
    "    \n",
    "    pad_top = max(-top, 0)\n",
    "    pad_left = max(-left, 0)\n",
    "    pad_bottom = max(size[1] - height - pad_top, 0)\n",
    "    pad_right = max(size[0] - width - pad_left, 0)\n",
    "\n",
    "    if isinstance(image, Image.Image):\n",
    "        padding = (pad_left, pad_top, pad_right, pad_bottom)\n",
    "        return ImageOps.expand(image, padding)\n",
    "    elif isinstance(image, torch.Tensor):\n",
    "        padding = (pad_left, pad_right, pad_top, pad_bottom)\n",
    "        return torch.nn.functional.pad(image, padding, mode='constant', value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_crop(\n",
    "    batch:torch.tensor, \n",
    "    size:typing.Tuple[int,int]\n",
    "):\n",
    "    \"\"\"\n",
    "    Crops each image in `batch` to a particular `size`.\n",
    "    \n",
    "    Args:\n",
    "        batch (array of `torch.Tensor`):\n",
    "            A batch of images, should be of shape `NxCxWxH`\n",
    "        size (`tuple` of integers):\n",
    "            A size to pad to, should be in the form\n",
    "            of (width, height)\n",
    "            \n",
    "    Returns:\n",
    "        A batch of cropped images\n",
    "    \"\"\"\n",
    "   \n",
    "    affine_matrix = torch.eye(3, device=batch.device).float()\n",
    "    affine_matrix = affine_matrix.unsqueeze(0)\n",
    "    affine_matrix = affine_matrix.expand(batch.size(0), 3, 3)\n",
    "    affine_matrix = affine_matrix.contiguous()[:,:2]\n",
    "    \n",
    "    coords = F.affine_grid(\n",
    "        affine_matrix, batch.shape[:2] + size, align_corners=True\n",
    "    )\n",
    "    \n",
    "    top_range, bottom_range = coords.min(), coords.max()\n",
    "    zoom = 1/(bottom_range - top_range).item()*2\n",
    "    \n",
    "    resizing_limit = min(\n",
    "        batch.shape[-2]/coords.shape[-2],\n",
    "        batch.shape[-1]/coords.shape[-1]\n",
    "    )/2\n",
    "    \n",
    "    if resizing_limit > 1 and resizing_limit > zoom:\n",
    "        batch = F.interpolate(\n",
    "            batch, \n",
    "            scale_factor=1/resizing_limit, \n",
    "            mode='area', \n",
    "            recompute_scale_factor=True\n",
    "        )\n",
    "    return F.grid_sample(batch, coords, mode='bilinear', padding_mode='reflection', align_corners=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_torch = tvtfms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'timm.models.convnext.ConvNeXt'>\n",
      "Predicted class index: 1\n",
      "Predicted class label: yes\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# define the labels as the came from dls.vocab\n",
    "class_labels = ['no', 'yes']\n",
    "\n",
    "# Define the image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image to the size expected by the model\n",
    "    transforms.ToTensor(),          # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "# Load the image\n",
    "image_path = 'processed/valid/yes/Y14.jpg'\n",
    "image = Image.open(image_path)\n",
    "# image = transform(image).unsqueeze(0)  # Add a batch dimension\n",
    "image = gpu_crop(tt_torch(image).unsqueeze(0), (224, 224))\n",
    "\n",
    "print(type(model))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Run the image through the model\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "\n",
    "# Get the predicted class\n",
    "_, predicted_class = torch.max(output, 1)\n",
    "predicted_index = predicted_class.item()\n",
    "\n",
    "# Map the index to the class label\n",
    "predicted_label = class_labels[predicted_index]\n",
    "\n",
    "print(f'Predicted class index: {predicted_index}')\n",
    "print(f'Predicted class label: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('18 no.jpg', 0, 'no'), ('27 no.jpg', 0, 'no'), ('4 no.jpg', 0, 'no'), ('42 no.jpg', 0, 'no'), ('N6.jpg', 0, 'no'), ('no 89.jpg', 0, 'no'), ('no 9.png', 0, 'no'), ('no 99.jpg', 0, 'no'), ('No14.jpg', 0, 'no')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define the labels as they came from dls.vocab\n",
    "class_labels = ['no', 'yes']\n",
    "\n",
    "# Define the image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image to the size expected by the model\n",
    "    transforms.ToTensor(),          # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "# Path to the \"no\" folder\n",
    "folder_path = 'processed/valid/no'\n",
    "\n",
    "# List all image files in the \"no\" folder\n",
    "image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Loop through each image file\n",
    "for image_file in image_files:\n",
    "    # print(f'Processing Image: {image_file}')\n",
    "    image_path = os.path.join(folder_path, image_file)\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # print(f'Image shape: {image.size}')\n",
    "    # print no of channels\n",
    "    # print(f'Image channels: {len(image.getbands())}')\n",
    "    \n",
    "    # Convert single-channel images to three-channel\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    image = gpu_crop(tt_torch(image).unsqueeze(0), (224, 224))\n",
    "    # image = transform(image).unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "    # Run the image through the model\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "\n",
    "    # Get the predicted class\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "    predicted_index = predicted_class.item()\n",
    "\n",
    "    # Map the index to the class label\n",
    "    predicted_label = class_labels[predicted_index]\n",
    "\n",
    "    predictions.append((image_file, predicted_index, predicted_label))\n",
    "    # print(f'Image: {image_file}')\n",
    "    # print(f'Predicted class index: {predicted_index}')\n",
    "    # print(f'Predicted class label: {predicted_label}')\n",
    "    # print('---')\n",
    "    \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Y108.jpg', 0, 'no'), ('Y109.JPG', 0, 'no'), ('Y14.jpg', 1, 'yes'), ('Y159.JPG', 0, 'no'), ('Y19.JPG', 1, 'yes'), ('Y22.jpg', 1, 'yes'), ('Y246.JPG', 1, 'yes'), ('Y250.jpg', 1, 'yes'), ('Y27.jpg', 1, 'yes'), ('Y33.jpg', 0, 'no'), ('Y34.jpg', 1, 'yes'), ('Y42.jpg', 0, 'no'), ('Y51.jpg', 1, 'yes'), ('Y56.jpg', 0, 'no'), ('Y65.JPG', 1, 'yes')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define the labels as they came from dls.vocab\n",
    "class_labels = ['no', 'yes']\n",
    "\n",
    "# Define the image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image to the size expected by the model\n",
    "    transforms.ToTensor(),          # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "# Path to the \"no\" folder\n",
    "folder_path = 'processed/valid/yes'\n",
    "\n",
    "# List all image files in the \"no\" folder\n",
    "image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Loop through each image file\n",
    "for image_file in image_files:\n",
    "    # print(f'Processing Image: {image_file}')\n",
    "    image_path = os.path.join(folder_path, image_file)\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # print(f'Image shape: {image.size}')\n",
    "    # print no of channels\n",
    "    # print(f'Image channels: {len(image.getbands())}')\n",
    "    \n",
    "    # Convert single-channel images to three-channel\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    image = gpu_crop(tt_torch(image).unsqueeze(0), (224, 224))\n",
    "    # image = transform(image).unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "    # Run the image through the model\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "\n",
    "    # Get the predicted class\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "    predicted_index = predicted_class.item()\n",
    "\n",
    "    # Map the index to the class label\n",
    "    predicted_label = class_labels[predicted_index]\n",
    "\n",
    "    predictions.append((image_file, predicted_index, predicted_label))\n",
    "    # print(f'Image: {image_file}')\n",
    "    # print(f'Predicted class index: {predicted_index}')\n",
    "    # print(f'Predicted class label: {predicted_label}')\n",
    "    # print('---')\n",
    "    \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Y108.jpg', 0, 'no'), ('Y109.JPG', 0, 'no'), ('Y14.jpg', 1, 'yes'), ('Y159.JPG', 0, 'no'), ('Y19.JPG', 1, 'yes'), ('Y22.jpg', 0, 'no'), ('Y246.JPG', 1, 'yes'), ('Y250.jpg', 1, 'yes'), ('Y27.jpg', 0, 'no'), ('Y33.jpg', 0, 'no'), ('Y34.jpg', 1, 'yes'), ('Y42.jpg', 0, 'no'), ('Y51.jpg', 0, 'no'), ('Y56.jpg', 0, 'no'), ('Y65.JPG', 1, 'yes')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define the labels as they came from dls.vocab\n",
    "class_labels = ['no', 'yes']\n",
    "\n",
    "# Define the image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image to the size expected by the model\n",
    "    transforms.ToTensor(),          # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "# Path to the \"no\" folder\n",
    "folder_path = 'processed/valid/yes'\n",
    "\n",
    "# List all image files in the \"no\" folder\n",
    "image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Loop through each image file\n",
    "for image_file in image_files:\n",
    "    # print(f'Processing Image: {image_file}')\n",
    "    image_path = os.path.join(folder_path, image_file)\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # print(f'Image shape: {image.size}')\n",
    "    # print no of channels\n",
    "    # print(f'Image channels: {len(image.getbands())}')\n",
    "    \n",
    "    # Convert single-channel images to three-channel\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    image = pad(crop(image, (224, 224)), (224, 224))\n",
    "    image = tvtfms.ToTensor()(image).unsqueeze(0)  # Add a batch dimension\n",
    "    image = gpu_crop(image, (224, 224))\n",
    "    # image = transform(image).unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "    # Run the image through the model\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "\n",
    "    # Get the predicted class\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "    predicted_index = predicted_class.item()\n",
    "\n",
    "    # Map the index to the class label\n",
    "    predicted_label = class_labels[predicted_index]\n",
    "\n",
    "    predictions.append((image_file, predicted_index, predicted_label))\n",
    "    # print(f'Image: {image_file}')\n",
    "    # print(f'Predicted class index: {predicted_index}')\n",
    "    # print(f'Predicted class label: {predicted_label}')\n",
    "    # print('---')\n",
    "\n",
    "# doing these transforms gives a terrible result    \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
